---
title: 'Amazon Machine Learning Conference (AMLC 2025)'
date: 2025-11-06
permalink: /posts/2025/11/amlc-2025/
tags:
  - science
---

I've spent the last few days in Seattle at Amazon’s internal Machine Learning Conference (AMLC). If last year was defined by the raw frontier of GenAI capabilities, this year the focus shifted decisively toward agents, reliability, and real-world deployment. The conversation has moved from _“Can we do X?”_ to _“How do we evaluate, govern, and safely operationalize X at scale?”_. It felt like a distinctly Amazonian event: pragmatic, execution-oriented, and full of hallway discussions about shipping real systems and delivering customer impact.

I participated in the _Machine Learning for Healthcare Roundtable_ and gave a talk on [LLM trustworthiness in medical product question answering](https://daniellopezmartinez.com/talks/2025-11-04-talk-amlc), drawing on my earlier work on [Rufus](https://www.aboutamazon.com/news/retail/amazon-rufus). But the real highlight was learning from the impressive work happening across teams. Below are a few themes that stood out to me.

<img src='/images/blog/2025-11-07-amlc.jpeg' width="700" height="400">


## Pattie Maes on human flourishing with AI

One of my favorite moments at AMLC was seeing [Pattie Maes](https://www.media.mit.edu/people/pattie/overview/) give a keynote. Pattie is a professor at the [MIT Media Lab](https://www.media.mit.edu/), where I did my PhD, and she has been thinking about agents and augmentation for decades—long before today’s wave of foundation models.


## Forecasting in the age of foundation models: Chronos-2

One of the most interesting set of talks centered around [Chronos-2](https://www.amazon.science/blog/introducing-chronos-2-from-univariate-to-universal-forecasting), Amazon’s new pretrained time-series forecasting model. Chronos-2 is positioned as a universal forecaster: the same model can be applied, zero-shot, to univariate series, multivariate series, and tasks that depend heavily on exogenous signals. Unlike earlier foundation models that mostly focused on univariate series, Chronos-2 uses a group attention mechanism to share information across related time series within a batch. In practice this means you can group series by “target + covariates,” and the model learns useful structure within those groups in-context—without task-specific fine-tuning. 

For health applications, this is interesting for a few reasons. Many of our forecasting problems are data-scarce at the individual entity level but rich in structure across entities. Chronos-2’s zero-shot and cross-learning capabilities could help us bootstrap reasonable forecasts without maintaining a zoo of bespoke models. Also, many clinical operational decisions depend heavily on covariates. Chronos-2 is explicitly built to exploit such signals, not treat them as afterthoughts.

## GenAI evaluation, LLM-as-judge, and trustworthy agents

A lot of the AMLC program this year revolved around evaluation paradigms for GenAI:

## Pattie Maes on human flourishing with AI

One of my favorite moments at AMLC was seeing Pattie Maes give a keynote. Pattie is a professor at the MIT Media Lab, where I did my PhD, and she has been thinking about agents and augmentation for decades—long before today’s wave of foundation models.

## Takeaways for health AI